{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `gymnax`: Classic Gym Environments in JAX\n",
    "### [Last Update: June 2022][![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RobertTLange/gymnax/blob/main/examples/getting_started.ipynb)\n",
    "\n",
    "Welcome to `gymnax`, the one stop shop for fast classic Reinforcement Learning environments powered by JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "!pip install -q git+https://github.com/RobertTLange/gymnax.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic API: `gymnax.make()`, `env.reset()`, `env.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " [GpuDevice(id=0, process_index=0),\n",
       "  GpuDevice(id=1, process_index=0),\n",
       "  GpuDevice(id=2, process_index=0),\n",
       "  GpuDevice(id=3, process_index=0)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the number of (emulated) host devices\n",
    "num_devices = 4\n",
    "os.environ['XLA_FLAGS'] = f\"--xla_force_host_platform_device_count={num_devices}\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import gymnax\n",
    "\n",
    "jax.device_count(), jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvParams(max_speed=8.0, max_torque=2.0, dt=0.05, g=10.0, m=1.0, l=1.0, max_steps_in_episode=200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key_reset, key_policy, key_step = jax.random.split(rng, 4)\n",
    "\n",
    "# Create the Pendulum-v1 environment\n",
    "env, env_params = gymnax.make(\"Pendulum-v1\")\n",
    "\n",
    "# Inspect default environment settings\n",
    "env_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get an overview of all implemented environments as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CartPole-v1',\n",
       " 'Pendulum-v1',\n",
       " 'Acrobot-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Asterix-MinAtar',\n",
       " 'Breakout-MinAtar',\n",
       " 'Freeway-MinAtar',\n",
       " 'SpaceInvaders-MinAtar',\n",
       " 'Catch-bsuite',\n",
       " 'DeepSea-bsuite',\n",
       " 'MemoryChain-bsuite',\n",
       " 'UmbrellaChain-bsuite',\n",
       " 'DiscountingChain-bsuite',\n",
       " 'MNISTBandit-bsuite',\n",
       " 'SimpleBandit-bsuite',\n",
       " 'FourRooms-misc',\n",
       " 'MetaMaze-misc',\n",
       " 'PointRobot-misc',\n",
       " 'BernoulliBandit-misc',\n",
       " 'GaussianBandit-misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gymnax.registered_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([-0.939326  , -0.34302574, -0.6520283 ], dtype=float32),\n",
       " EnvState(theta=DeviceArray(-2.7914565, dtype=float32), theta_dot=DeviceArray(-0.6520283, dtype=float32), last_u=DeviceArray(0., dtype=float32, weak_type=True), time=DeviceArray(0, dtype=int32, weak_type=True)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, state = env.reset(key_reset, env_params)\n",
    "obs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([-0.94944364, -0.31393763, -0.6159719 ], dtype=float32),\n",
       " EnvState(theta=DeviceArray(-2.8222551, dtype=float32), theta_dot=DeviceArray(-0.6159719, dtype=float32), last_u=DeviceArray(1.9555049, dtype=float32), time=DeviceArray(1, dtype=int32, weak_type=True)),\n",
       " DeviceArray([-7.8385677], dtype=float32),\n",
       " DeviceArray(False, dtype=bool, weak_type=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = env.action_space(env_params).sample(key_policy)\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action, env_params)\n",
    "n_obs, n_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also simply use the environment with its default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, state = env.reset(key_reset)\n",
    "action = env.action_space().sample(key_policy)\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gymnax` provides fully functional environment dynamics that can leverage the full power of JAX's function transformations. E.g. one common RL use-case the parallel rollout of multiple workers. Using a `vmap` across random seeds (one per worker) allows us to implement such a parallelization on a single machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3)\n"
     ]
    }
   ],
   "source": [
    "vmap_reset = jax.vmap(env.reset, in_axes=(0, None))\n",
    "vmap_step = jax.vmap(env.step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "num_envs = 8\n",
    "vmap_keys = jax.random.split(rng, num_envs)\n",
    "\n",
    "obs, state = vmap_reset(vmap_keys, env_params)\n",
    "n_obs, n_state, reward, done, _ = vmap_step(vmap_keys, state, jnp.zeros(num_envs), env_params)\n",
    "print(n_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can also choose to `pmap` across rollout workers (\"actors\") across multiple devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "pmap_reset = jax.pmap(env.reset, in_axes=(0, None))\n",
    "pmap_step = jax.vmap(env.step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "\n",
    "pmap_keys = jax.random.split(rng, num_devices)\n",
    "obs, state = pmap_reset(pmap_keys, env_params)\n",
    "n_obs, n_state, reward, done, _ = pmap_step(pmap_keys, state, jnp.zeros(num_devices), env_params)\n",
    "print(n_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has executed each worker-specific environment transition on a separate device, but we can also chain `vmap` and `pmap` to execute multiple workers on a single device and at the same time across multiple devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 3)\n"
     ]
    }
   ],
   "source": [
    "map_reset = jax.pmap(vmap_reset, in_axes=(0, None))\n",
    "map_step = jax.pmap(vmap_step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "map_keys = jnp.tile(vmap_keys, (num_devices, 1, 1))\n",
    "obs, state = map_reset(map_keys, env_params)\n",
    "n_obs, n_state, reward, done, _ = map_step(map_keys, state, jnp.zeros((num_devices, num_envs)), env_params)\n",
    "print(n_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily leverage massive accelerator parallelism to churn through millions/billions of environment transitions when training 'sentient' agents. Note that in the code snippet above we have executed 4 times the same 8 environment workers, since we tiled/repeated the same key across the device axis. In general `pmap`-ing will require you to pay special attention to the shapes of the arrays that come out your operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jitted Episode Rollouts via `lax.scan`\n",
    "\n",
    "Let's now walk through an example of using `gymnax` with one of the common neural network libraries to parametrize a simple policy: `flax`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple ReLU MLP.\"\"\"\n",
    "\n",
    "    num_hidden_units: int\n",
    "    num_hidden_layers: int\n",
    "    num_output_units: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, rng):\n",
    "        for l in range(self.num_hidden_layers):\n",
    "            x = nn.Dense(features=self.num_hidden_units)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.num_output_units)(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = MLP(48, 1, 1)\n",
    "policy_params = model.init(rng, jnp.zeros(3), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(rng_input, policy_params, env_params, steps_in_episode):\n",
    "    \"\"\"Rollout a jitted gymnax episode with lax.scan.\"\"\"\n",
    "    # Reset the environment\n",
    "    rng_reset, rng_episode = jax.random.split(rng_input)\n",
    "    obs, state = env.reset(rng_reset, env_params)\n",
    "\n",
    "    def policy_step(state_input, tmp):\n",
    "        \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
    "        obs, state, policy_params, rng = state_input\n",
    "        rng, rng_step, rng_net = jax.random.split(rng, 3)\n",
    "        action = model.apply(policy_params, obs, rng_net)\n",
    "        next_obs, next_state, reward, done, _ = env.step(\n",
    "          rng_step, state, action, env_params\n",
    "        )\n",
    "        carry = [next_obs, next_state, policy_params, rng]\n",
    "        return carry, [obs, action, reward, next_obs, done]\n",
    "\n",
    "    # Scan over episode step loop\n",
    "    _, scan_out = jax.lax.scan(\n",
    "      policy_step,\n",
    "      [obs, state, policy_params, rng_episode],\n",
    "      (),\n",
    "      steps_in_episode\n",
    "    )\n",
    "    # Return masked sum of rewards accumulated by agent in episode\n",
    "    obs, action, reward, next_obs, done = scan_out\n",
    "    return obs, action, reward, next_obs, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 3), (200, 1), DeviceArray(-1600.6174, dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jit-Compiled Episode Rollout\n",
    "jit_rollout = jax.jit(rollout, static_argnums=3)\n",
    "obs, action, reward, next_obs, done = jit_rollout(rng, policy_params, env_params, 200)\n",
    "obs.shape, reward.shape, jnp.sum(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can wrap this `rollout` function with the magic of JAX and for all implemented RL environments. But we also provide a simple that does so for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnax.experimental import RolloutWrapper\n",
    "\n",
    "# Define rollout manager for pendulum env\n",
    "manager = RolloutWrapper(model.apply, env_name=\"Pendulum-v1\")\n",
    "\n",
    "# Simple single episode rollout for policy\n",
    "obs, action, reward, next_obs, done, cum_ret = manager.single_rollout(rng, policy_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple rollouts for same network (different rng, e.g. eval)\n",
    "rng_batch = jax.random.split(rng, 10)\n",
    "obs, action, reward, next_obs, done, cum_ret = manager.batch_rollout(\n",
    "    rng_batch, policy_params\n",
    ")\n",
    "\n",
    "# Multiple rollouts for different networks + rng (e.g. for ES)\n",
    "batch_params = jax.tree_map(  # Stack parameters or use different\n",
    "    lambda x: jnp.tile(x, (5, 1)).reshape(5, *x.shape), policy_params\n",
    ")\n",
    "obs, action, reward, next_obs, done, cum_ret = manager.population_rollout(\n",
    "    rng_batch, batch_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Episode Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnax.visualize import Visualizer\n",
    "\n",
    "state_seq, reward_seq = [], []\n",
    "rng, rng_reset = jax.random.split(rng)\n",
    "obs, env_state = env.reset(rng_reset, env_params)\n",
    "t_counter = 0\n",
    "while True:\n",
    "    state_seq.append(env_state)\n",
    "    rng, rng_act, rng_step = jax.random.split(rng, 3)\n",
    "    action = env.action_space(env_params).sample(rng_act)\n",
    "    next_obs, next_env_state, reward, done, info = env.step(\n",
    "        rng_step, env_state, action, env_params\n",
    "    )\n",
    "    reward_seq.append(reward)\n",
    "    t_counter += 1\n",
    "    if done or t_counter >= 50:\n",
    "        break\n",
    "    else:\n",
    "        obs = next_obs\n",
    "        env_state = next_env_state\n",
    "\n",
    "cum_rewards = jnp.cumsum(jnp.array(reward_seq))\n",
    "vis = Visualizer(env, env_params, state_seq, cum_rewards)\n",
    "vis.animate(f\"../docs/anim.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../docs/anim.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='../docs/anim.gif')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snippets",
   "language": "python",
   "name": "snippets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
